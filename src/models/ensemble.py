import copy

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error

from ..utils import print_header

class StackedEnsembleForecaster:
    '''
    A meta-forecaster that combines predictions from multiple base models
    (e.g., GRU, XGBoost) using a stacking strategy.

    It works in two main stages:
    1.  fit: Trains a 'meta-model' (e.g., Linear Regression) on the
        out-of-fold predictions generated by the base models on the
        validation set.
    2.  evaluate/predict: Re-trains the base models on the full
        (train + valid) dataset to get final predictions, which are then
        fed into the trained meta-model.
    '''
    def __init__(self, base_models, meta_model):
        '''
        Initializes the StackedForecaster.

        Args:
            base_forecasters (list): A list of initialized base forecaster
                instances (e.g., [gru_forecaster, xgb_forecaster]). These
                forecasters should not be trained yet.
            meta_model (sklearn.base.RegressorMixin, optional): An
                un-fitted scikit-learn regressor to use as the meta-model.
                Defaults to LinearRegression.
        '''
        self.base_models = base_models
        self.meta_model = meta_model if meta_model else LinearRegression()

        self.model_names = [f'forecaster_{m.__class__.__name__.replace("Forecaster", "").lower()}'
                            for m in self.base_models]

    def _calculate_mape(self, true, pred):
        '''
        Calculates the Mean Absolute Percentage Error (MAPE).
        
        Args:
            true (pd.Series): The Series of the truth values.
            pred (pd.Series): The Series of the predicted values.

        Returns:
            float: The MAPE value as a percentage.
        '''
        return mean_absolute_percentage_error(true, pred) * 100
    
    def _calculate_da(self, comp, history, forecast_horizon):
        '''
        Calculates the Directional Accuracy (DA).

        For a forecast made at time `t` for a future time `t + forecast_horizon`,
        this function defines:
        - The `baselines` as the actual values at time `t`.
        - The true direction as sign (value at `t + forecast_horizon` - baselines).
        - The predicted direction as sign (prediction for `t + forecast_horizon` - baselines).

        Args:
            comp (pd.DataFrame): The DataFrame containing the actual and predicted values for comparison.
            history (pd.Series): The Series containing the complete, true historical values of the time series.
            forecast_horizon (int): The number of steps to forecast.

        Returns:
            float: A DA value, returned as a percentage.            
                Returns 50.0 if the comp DataFrame becomes empty after handling NaNs.
        '''
        comp_copy = comp.copy()
        baselines = history.shift(forecast_horizon)
        comp_copy['baseline'] = baselines
        comp_copy.dropna(inplace=True)

        if comp_copy.empty:
            return 0.0

        comp_copy['true_dir'] = np.sign(comp_copy['true'] - comp_copy['baseline'])
        comp_copy['pred_dir'] = np.sign(comp_copy['pred'] - comp_copy['baseline'])
        
        correct_preds = (comp_copy['true_dir'] == comp_copy['pred_dir']).sum()

        return correct_preds / len(comp_copy) * 100
    
    def fit(self, forecast_horizon, base_model_params):
        '''
        Trains the meta-model.

        It first tells each base forecaster to `.fit()`, which trains
        them on their `train_ds` (using `valid_ds` for early stopping).
        Then, it gets the validation predictions from each by calling
        `.evaluate()` and uses these predictions to train the meta-model.

        Args:
            forecast_horizon (int): The specific forecast horizon to use
                for the meta-model's training labels.
            base_model_params (dict): A dictionary mapping model names
                to the arguments their .fit() method requires.
                Example:
                {
                    'forecaster_gru': {'model': gru_hypermodel, 'epochs': 500},
                    'forecaster_xgb': {'params': xgb_params}
                }        
        '''        
        print_header('Step 1: Fitting Stacked Ensemble Model')

        X_train_meta = dict()
        y_train_meta = None

        for i, model in enumerate(self.base_models):
            model_name = self.model_names[i]

            if model_name not in base_model_params:
                raise ValueError(f'Missing parameters for \'{model_name}\' in \'base_model_params\'.')

            model_param = base_model_params[model_name]
            model.fit(**model_param)
                
            metrics = model.evaluate(forecast_horizon)

            X_train_meta[model_name] = metrics['comp']['pred']
            y_train_meta = metrics['comp']['true']
        
        print('--- Fitting Meta-Model ---')
        self.meta_model.fit(pd.DataFrame(X_train_meta), y_train_meta)

        print('\n--- Meta-Model Training Complete ---')
        try:
            print(f'- Meta-Model Coefficients ({self.model_names}): {self.meta_model.coef_}')
            print(f'- Meta-Model Intercept: {self.meta_model.intercept_}\n')
        except AttributeError:
            pass

    def evaluate(self, test_ds, forecast_horizon, base_model_params, time_unit):
        '''
        Evaluates the stacked pipeline on the test set.

        This method re-trains all base models on the combined (train + valid) dataset.
        It then gets their predictions on `test_ds`, feeds those predictions into 
        the (already-trained) meta-model, and computes the stacked metrics.

        Args:
            test_ds (pd.DataFrame): The unseen test dataset.
            forecast_horizon (int): The number of steps to forecast.
            base_model_params (dict): A dictionary mapping model names
                to the arguments their .evaluate() method requires. This is
                needed to re-train the models on the full (train + valid) data.
            time_unit (str): The time unit for the forecast horizon.

        Returns:
            dict: A dictionary containing the performance metrics (`mape`, `da`) and a
                comparison DataFrame (`comp`) with the actual vs. predicted values.
        '''        
        print_header(f'Step 2: Evaluating Stacked Ensemble Model on Test Set (Horizon: {forecast_horizon} {time_unit})')

        X_test_meta = dict()
        y_test_meta = None
        self.base_models_retrained = list()

        for i, model in enumerate(self.base_models):
            model_name = self.model_names[i]

            if model_name not in base_model_params:
                raise ValueError(f'Missing parameters for \'{model_name}\' in \'base_model_params\'.')

            model_retrained = copy.deepcopy(model)
            
            train_full_ds = pd.concat([model.train_ds, model.valid_ds])

            model_retrained.train_ds = train_full_ds
            model_retrained.valid_ds = test_ds

            model_param = base_model_params[model_name]
            model_retrained.fit(**model_param)

            metrics = model_retrained.evaluate(forecast_horizon)

            X_test_meta[model_name] = metrics['comp']['pred']
            y_test_meta = metrics['comp']['true']

            self.base_models_retrained.append(model_retrained)

        print('\n--- Evaluating Stacked Ensemble Model ---')
        stacked_preds = self.meta_model.predict(pd.DataFrame(X_test_meta))

        history_src = self.base_models_retrained[0]
        history = pd.concat([history_src.train_ds, history_src.valid_ds])[history_src.target_col]
                            
        comp = pd.DataFrame({
            'true': y_test_meta,
            'pred': stacked_preds
        }, index=y_test_meta.index)

        mape = self._calculate_mape(y_test_meta, stacked_preds)
        da = self._calculate_da(comp, history, forecast_horizon)

        print(f'- Mean Absolute Percentage Error (MAPE): {mape:.4f}%')
        print(f'- Directional Accuracy (DA): {da:.4f}%\n')

        return {
            'mape': mape,
            'da': da,
            'comp': comp
        }

    def predict(self, forecast_horizon, base_model_params, hold_thld=0.005):
        '''
        Generates a single, final stacked forecast for the future.
        
        This method must be called after .evaluate(), as it relies on the
        `base_models_retrained` models (which were trained on
        train + valid data) to generate its inputs.

        Args:
            forecast_horizon (int): The number of steps to forecast.
            base_model_params (dict): A dictionary mapping model names
                to the arguments their .predict() method requires.
                This is needed to pass the compiled Keras model to
                the LSTM/RNN forecaster.
            hold_thld (float, optional): The percentage threshold for the hold signal.

        Returns:
            dict: A dictionary containing the following keys:
                - pred (float): The single, predicted value for the date
                                 `forecast_horizon` steps after the last known data point.
                - sig (float): The sign of the directional signal 
                                 (1.0 for up, -1.0 for down, 0.0 for hold).
        
        Raises:
            RuntimeError: If called before 'evaluate' has been run.
        '''
        print_header(f'Step 3: Generating Final Stacked Ensemble (Threshold: {hold_thld * 100}%) Forecast')
        if not self.base_models_retrained:
            raise RuntimeError('Must run .evaluate() before .predict() to train the final base models.')
            
        X_pred_meta = dict()
        last_value = None

        for i, model in enumerate(self.base_models_retrained):
            model_name = self.model_names[i]

            if model_name not in base_model_params:
                raise ValueError(f'Missing parameters for \'{model_name}\' in \'base_model_params\'.')

            model_param = base_model_params[model_name]

            pred_out = None

            if 'model' in model_param:
                pred_out = model.predict(model_param['model'], forecast_horizon, hold_thld)
            else:
                pred_out = model.predict(forecast_horizon, hold_thld)
            
            X_pred_meta[model_name] = [pred_out['pred']]
            
            history = pd.concat([model.train_ds, model.valid_ds])
            last_value = history[model.target_col].iloc[-1]

        print(f'\n--- Generating Final Stacked Ensemble (Threshold: {hold_thld * 100}%) Forecast ---')

        pred = self.meta_model.predict(pd.DataFrame(X_pred_meta))[0]
        
        if last_value == 0:
            diff = pred - last_value
            sig = np.sign(diff)
        else:
            pct_change = (pred - last_value) / last_value
            
            if pct_change > hold_thld:
                sig = 1.0
            elif pct_change < -hold_thld:
                sig = -1.0
            else:
                sig = 0.0

        history_src = self.base_models_retrained[0]
        history = pd.concat([history_src.train_ds, history_src.valid_ds])

        last_time = history.index[-1]
        freq = pd.infer_freq(history.index)
        future_time = pd.date_range(start=last_time, periods=forecast_horizon + 1, freq=freq)[-1]
        formatted_future_time = future_time.date().strftime('%Y-%m-%d')

        print(f'- Forecast for {formatted_future_time}: ${pred:.2f}')
        print(f'- Directional Signal for {formatted_future_time}: {sig:.1f}')
        
        return {'pred': pred, 'sig': sig}